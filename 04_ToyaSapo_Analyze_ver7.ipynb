{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ToyaSapo å•ã„åˆã‚ã›ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆçµ±åˆç‰ˆï¼‰\n",
                "\n",
                "## ğŸš€ è‡ªå‹•åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ\n",
                "- **ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼**: Google Driveã‹ã‚‰è‡ªå‹•èª­ã¿è¾¼ã¿\n",
                "- **Azureã‚­ãƒ¼**: Google Driveã‹ã‚‰è‡ªå‹•èª­ã¿è¾¼ã¿\n",
                "- **ãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿**: Firestoreã‹ã‚‰ç›´æ¥å–å¾—ï¼ˆCSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸è¦ï¼‰\n",
                "\n",
                "## ğŸ”§ åˆå›ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆ1å›ã ã‘ï¼‰\n",
                "1. Google Driveã® `001_PBL_ãƒãƒ¼ãƒ D_AI4th` ãƒ•ã‚©ãƒ«ãƒ€ã«ä»¥ä¸‹ã‚’é…ç½®\n",
                "   - `toyasapo-firebase-adminsdk-fbsvc-93683c6f93.json`\n",
                "   - `azure_textanalytics_key.txt`\n",
                "\n",
                "## ğŸ“ æ¯å›ã®ä½¿ã„æ–¹\n",
                "1. ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’é–‹ã\n",
                "2. ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã€â†’ã€Œã™ã¹ã¦ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã€\n",
                "3. Google Driveã®ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ã‚’æ‰¿èª\n",
                "4. å®Œäº†ï¼\n",
                "\n",
                "## ğŸ’¾ ä¿å­˜ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿\n",
                "`analysis_results/latest` ã«ä»¥ä¸‹ãŒä¿å­˜ã•ã‚Œã¾ã™ï¼š\n",
                "- `updated_at`: æ›´æ–°æ—¥æ™‚\n",
                "- `total_count`: ç·ä»¶æ•°\n",
                "- `keywords_top20`: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰TOP20\n",
                "- `keywords_tfidf`: é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆTF-IDFï¼‰\n",
                "- `bigrams_top20`: N-gramï¼ˆ2èªã®çµ„ã¿åˆã‚ã›ï¼‰\n",
                "- `cooccurrence`: å…±èµ·åˆ†æ\n",
                "- `disease`: ç—…åãƒ»è¨ºæ–­\n",
                "- `symptoms`: ç—‡çŠ¶\n",
                "- `concerns`: æ‚©ã¿ãƒ»çŠ¶æ³\n",
                "- `inquiry_types`: ç›¸è«‡å†…å®¹\n",
                "- `sentiment_summary`: æ„Ÿæƒ…åˆ†æï¼ˆpositive/negative/neutral/mixedï¼‰"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨Driveæ¥ç¶šï¼‰"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
                "!apt-get -y install fonts-ipafont-gothic > /dev/null 2>&1\n",
                "!pip install janome azure-ai-textanalytics japanize-matplotlib firebase-admin -q\n",
                "\n",
                "print(\"âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import japanize_matplotlib\n",
                "from collections import Counter, defaultdict\n",
                "from janome.tokenizer import Tokenizer\n",
                "from datetime import datetime\n",
                "import warnings\n",
                "import os\n",
                "import json\n",
                "import numpy as np\n",
                "from statistics import mean\n",
                "\n",
                "# Azure & Firebase\n",
                "import firebase_admin\n",
                "from firebase_admin import credentials, firestore\n",
                "from azure.ai.textanalytics import TextAnalyticsClient\n",
                "from azure.core.credentials import AzureKeyCredential\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š\n",
                "plt.rcParams['font.family'] = 'IPAPGothic'\n",
                "japanize_matplotlib.japanize()\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿å®Œäº†\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "print(\"âœ… Google Drive ãƒã‚¦ãƒ³ãƒˆå®Œäº†\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. èªè¨¼æƒ…å ±ã¨FirebaseåˆæœŸåŒ–"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- è¨­å®š ---\n",
                "BASE_PATH = '/content/drive/MyDrive/001_PBL_ãƒãƒ¼ãƒ D_AI4th'\n",
                "FIREBASE_KEY_PATH = os.path.join(BASE_PATH, 'toyasapo-firebase-adminsdk-fbsvc-93683c6f93.json')\n",
                "AZURE_KEY_PATH = os.path.join(BASE_PATH, 'azure_textanalytics_key.txt')\n",
                "AZURE_ENDPOINT = \"https://toyasapo-language.cognitiveservices.azure.com/\"\n",
                "\n",
                "# 1. FirebaseåˆæœŸåŒ–\n",
                "if not os.path.exists(FIREBASE_KEY_PATH):\n",
                "    print(\"âŒ ã‚¨ãƒ©ãƒ¼: firebase-key.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
                "    print(f\"   ãƒ‘ã‚¹: {FIREBASE_KEY_PATH}\")\n",
                "else:\n",
                "    if not firebase_admin._apps:\n",
                "        try:\n",
                "            cred = credentials.Certificate(FIREBASE_KEY_PATH)\n",
                "            firebase_admin.initialize_app(cred)\n",
                "            print(\"âœ… FirebaseåˆæœŸåŒ–å®Œäº†\")\n",
                "        except Exception as e:\n",
                "            print(f\"âŒ FirebaseåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}\")\n",
                "    else:\n",
                "        print(\"âœ… Firebaseæ—¢ã«åˆæœŸåŒ–æ¸ˆã¿\")\n",
                "    \n",
                "    db = firestore.client()\n",
                "    print(\"âœ… Firestoreæ¥ç¶šå®Œäº†\")\n",
                "\n",
                "# 2. Azure Keyèª­ã¿è¾¼ã¿\n",
                "azure_key = None\n",
                "if os.path.exists(AZURE_KEY_PATH):\n",
                "    try:\n",
                "        with open(AZURE_KEY_PATH, 'r') as f:\n",
                "            azure_key = f.read().strip()\n",
                "        print(\"âœ… Azure APIã‚­ãƒ¼èª­ã¿è¾¼ã¿å®Œäº†\")\n",
                "    except Exception as e:\n",
                "        print(f\"âŒ Azure Keyèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
                "else:\n",
                "    print(\"âŒ ã‚¨ãƒ©ãƒ¼: azure_textanalytics_key.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"ğŸ“§ Firestoreã‹ã‚‰ãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\")\n",
                "\n",
                "emails_ref = db.collection('emails')\n",
                "docs = emails_ref.stream()\n",
                "\n",
                "data = []\n",
                "for doc in docs:\n",
                "    doc_data = doc.to_dict()\n",
                "    if not doc_data.get('isDeleted', False):\n",
                "        data.append(doc_data)\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "\n",
                "print(f\"âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {len(df)}ä»¶\")\n",
                "\n",
                "# ã‚«ãƒ©ãƒ ç‰¹å®š\n",
                "content_col = None\n",
                "for col in ['inquiry', 'content', 'body', 'æœ¬æ–‡', 'æœ¬æ–‡(åŒ¿ååŒ–æ¸ˆ)', 'Content', 'Body', 'message']:\n",
                "    if col in df.columns:\n",
                "        content_col = col\n",
                "        break\n",
                "\n",
                "print(f\"æœ¬æ–‡ã‚«ãƒ©ãƒ : {content_col}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. å½¢æ…‹ç´ è§£æãƒ»TF-IDFãƒ»N-gramåˆ†æ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer = Tokenizer()\n",
                "\n",
                "stop_words = {\n",
                "    'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ã‚ˆã†', 'ã¨ã“ã‚', 'ã¨ã', 'æ–¹', 'äºº', 'ç§', 'è‡ªåˆ†',\n",
                "    'ä»Š', 'å¾Œ', 'å‰', 'ä¸­', 'ä¸Š', 'ä¸‹', 'ä»¶', 'ç‚¹', 'æ—¨', 'ç­‰', 'é ƒ', 'éš›',\n",
                "    'åº¦', 'åˆ†', 'å›', 'æœˆ', 'æ—¥', 'å¹´', 'æ™‚', 'é€±', 'å…ˆ', 'æ¬¡', 'åˆ¥', 'ä»–',\n",
                "    'ä»¥ä¸Š', 'ä»¥ä¸‹', 'ä»¥å‰', 'ä»¥é™', 'å ´åˆ', 'çŠ¶æ…‹', 'çŠ¶æ³', 'ç†ç”±', 'åŸå› ',\n",
                "    'ç¢ºèª', 'è³ªå•', 'ç›¸è«‡', 'ä¾é ¼', 'å¸Œæœ›', 'è¦æœ›', 'å¯¾å¿œ', 'è¿”ç­”',\n",
                "    'å®›å', 'å†’é ­', 'æ–‡æœ«', 'æœ¬æ–‡', 'ä»¶å', 'å†…å®¹', 'ä¸‹è¨˜', 'ä¸Šè¨˜',\n",
                "    'å¯èƒ½', 'å¿…è¦', 'å¤§å¤‰', 'æ‰¿çŸ¥', 'äº†è§£', 'ç”³ã—è¨³', 'æã‚Œå…¥ã‚Š',\n",
                "    'ã“ã¡ã‚‰', 'ãã¡ã‚‰', 'æ™‚é–“', 'ç´¹ä»‹', 'å¤‰æ›´',\n",
                "    'ã‚ã¡ã‚‰', 'ã©ã¡ã‚‰',\n",
                "    'ãƒ¡ãƒ¼ãƒ«', 'ã‚¢ãƒ‰ãƒ¬ã‚¹', 'å•ã„åˆã‚ã›', 'å•åˆã›', 'é€£çµ¡', 'è¿”ä¿¡', 'é€ä¿¡',\n",
                "    'å®›å…ˆ', 'ãŠå•ã„åˆã‚ã›', 'ãƒ•ã‚©ãƒ¼ãƒ ', 'ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹',\n",
                "    'ãŠä¸–è©±', 'ãŠé¡˜ã„', 'ã”é€£çµ¡', 'ã”ç¢ºèª', 'ã”è³ªå•', 'ã”ç›¸è«‡',\n",
                "    'ã¯ã˜ã‚ã¾ã—ã¦', 'åˆã‚ã¾ã—ã¦', 'ã‚ˆã‚ã—ã', 'å®œã—ã', 'çš†æ§˜', 'çš†ã•ã¾',\n",
                "    'toyano', 'mental', 'TMC', 'toya', 'ã¨ã‚„ã®', 'ãƒ¡ãƒ³ã‚¿ãƒ«', 'ã‚¯ãƒªãƒ‹ãƒƒã‚¯',\n",
                "    'niigata', 'æ–°æ½Ÿ',\n",
                "    'facebook', 'twitter', 'instagram', 'LINE', 'line', 'http', 'https', 'www',\n",
                "    'åå‰', 'å‰Šé™¤', 'æ‹…å½“', 'è€…', 'æ§˜', 'æ°', 'æ®¿', 'å¾¡ä¸­', 'ã•ã‚“', 'å…ˆç”Ÿ',\n",
                "    'é›»è©±', 'ç•ªå·', 'ä½æ‰€', 'ç¾åœ¨', 'æœ€è¿‘',\n",
                "}\n",
                "\n",
                "def extract_nouns(text):\n",
                "    if pd.isna(text):\n",
                "        return []\n",
                "    nouns = []\n",
                "    for token in tokenizer.tokenize(str(text)):\n",
                "        if token.part_of_speech.startswith('åè©'):\n",
                "            word = token.surface\n",
                "            if (len(word) >= 2 and\n",
                "                word not in stop_words and\n",
                "                not word.isascii() and\n",
                "                not word.isdigit() and\n",
                "                not any(c in word for c in '[](){}ã€Œã€ã€ã€‘ã€ã€‚ãƒ»>=</:\\\"')):\n",
                "                nouns.append(word)\n",
                "    return nouns\n",
                "\n",
                "print(\"å½¢æ…‹ç´ è§£æä¸­...\")\n",
                "all_nouns = []\n",
                "if content_col:\n",
                "    for text in df[content_col]:\n",
                "        all_nouns.extend(extract_nouns(text))\n",
                "    noun_counts = Counter(all_nouns)\n",
                "    print(f\"âœ… å®Œäº†: {len(all_nouns)}å€‹ã®åè©ã‚’æŠ½å‡º\")\n",
                "else:\n",
                "    print(\"âš ï¸ æœ¬æ–‡ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
                "    noun_counts = Counter()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TF-IDF\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "def tokenize_to_nouns(text):\n",
                "    nouns = extract_nouns(text)\n",
                "    return ' '.join(nouns)\n",
                "\n",
                "tfidf_ranking = []\n",
                "if content_col and len(df) > 0:\n",
                "    texts = df[content_col].fillna('').apply(tokenize_to_nouns).tolist()\n",
                "    vectorizer = TfidfVectorizer(max_features=100)\n",
                "    try:\n",
                "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
                "        feature_names = vectorizer.get_feature_names_out()\n",
                "        mean_tfidf = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
                "        tfidf_ranking = sorted(zip(feature_names, mean_tfidf), key=lambda x: x[1], reverse=True)\n",
                "        print(\"âœ… TF-IDFè¨ˆç®—å®Œäº†\")\n",
                "    except ValueError:\n",
                "        print(\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚TF-IDFã‚¹ã‚­ãƒƒãƒ—\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# N-gram\n",
                "def extract_bigrams(text):\n",
                "    nouns = extract_nouns(text)\n",
                "    if len(nouns) < 2:\n",
                "        return []\n",
                "    bigrams = []\n",
                "    for i in range(len(nouns) - 1):\n",
                "        if nouns[i] != nouns[i+1]:\n",
                "            bigram = f\"{nouns[i]}_{nouns[i+1]}\"\n",
                "            bigrams.append(bigram)\n",
                "    return bigrams\n",
                "\n",
                "all_bigrams = []\n",
                "if content_col:\n",
                "    for text in df[content_col]:\n",
                "        all_bigrams.extend(extract_bigrams(text))\n",
                "    bigram_counts = Counter(all_bigrams)\n",
                "    print(\"âœ… N-gramæŠ½å‡ºå®Œäº†\")\n",
                "else:\n",
                "    bigram_counts = Counter()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# å…±èµ·åˆ†æ\n",
                "def get_cooccurrence(texts, target_word):\n",
                "    cooccur = Counter()\n",
                "    for text in texts:\n",
                "        nouns = extract_nouns(text)\n",
                "        if target_word in nouns:\n",
                "            for noun in nouns:\n",
                "                if noun != target_word:\n",
                "                    cooccur[noun] += 1\n",
                "    return cooccur\n",
                "\n",
                "target_words = ['ã†ã¤', 'ç™ºé”', 'äºˆç´„', 'ä¸å®‰', 'ä»•äº‹', 'ä¼‘è·']\n",
                "cooccurrence_results = {}\n",
                "\n",
                "if content_col:\n",
                "    texts = df[content_col]\n",
                "    for target in target_words:\n",
                "        cooccur = get_cooccurrence(texts, target)\n",
                "        cooccurrence_results[target] = cooccur.most_common(10)\n",
                "    print(f\"âœ… å…±èµ·åˆ†æå®Œäº† ({len(target_words)}èª)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æï¼ˆåŒ»ç™‚ãƒ»ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ˜ãƒ«ã‚¹ï¼‰"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "keywords_dict = {\n",
                "    'ç—…åãƒ»è¨ºæ–­': [\n",
                "        'ã†ã¤ç—…', 'ã†ã¤', 'é¬±', 'é¬±ç—…', 'æŠ‘ã†ã¤', 'å¤§ã†ã¤',\n",
                "        'åŒæ¥µæ€§éšœå®³', 'åŒæ¥µæ€§', 'èºã†ã¤', 'èºé¬±', 'æ°—åˆ†éšœå®³', 'èºçŠ¶æ…‹',\n",
                "        'ãƒ‘ãƒ‹ãƒƒã‚¯éšœå®³', 'ãƒ‘ãƒ‹ãƒƒã‚¯', 'ç¤¾äº¤ä¸å®‰', 'ç¤¾ä¼šä¸å®‰', 'å¯¾äººææ€–', 'åºƒå ´ææ€–', 'å…¨èˆ¬æ€§ä¸å®‰',\n",
                "        'ç™ºé”éšœå®³', 'ADHD', 'æ³¨æ„æ¬ é™¥', 'ASD', 'è‡ªé–‰ç—‡', 'ã‚¢ã‚¹ãƒšãƒ«ã‚¬ãƒ¼', 'å­¦ç¿’éšœå®³', 'LD',\n",
                "        'é©å¿œéšœå®³', 'PTSD', 'å¿ƒçš„å¤–å‚·', 'ãƒˆãƒ©ã‚¦ãƒ', 'æ€¥æ€§ã‚¹ãƒˆãƒ¬ã‚¹',\n",
                "        'ä¸çœ ç—‡', 'ç¡çœ éšœå®³', 'éçœ ', 'ç¡çœ æ™‚ç„¡å‘¼å¸',\n",
                "        'å¼·è¿«æ€§éšœå®³', 'å¼·è¿«ç—‡', 'OCD', 'è§£é›¢æ€§éšœå®³', 'è§£é›¢',\n",
                "        'æ‘‚é£Ÿéšœå®³', 'éé£Ÿ', 'æ‹’é£Ÿ', 'éé£Ÿç—‡', 'æ‹’é£Ÿç—‡',\n",
                "        'ä¾å­˜ç—‡', 'ã‚¢ãƒ«ã‚³ãƒ¼ãƒ«ä¾å­˜', 'ã‚²ãƒ¼ãƒ ä¾å­˜', 'ã‚®ãƒ£ãƒ³ãƒ–ãƒ«ä¾å­˜',\n",
                "        'äººæ ¼éšœå®³', 'ãƒ‘ãƒ¼ã‚½ãƒŠãƒªãƒ†ã‚£éšœå®³', 'BPD', 'å¢ƒç•Œæ€§',\n",
                "        'çµ±åˆå¤±èª¿ç—‡', 'èªçŸ¥ç—‡', 'æ›´å¹´æœŸéšœå®³', 'è‡ªå¾‹ç¥çµŒå¤±èª¿ç—‡',\n",
                "        'PMS', 'PMDD', 'HSP', 'SAD', 'å­£ç¯€æ€§'\n",
                "    ],\n",
                "    'ç—‡çŠ¶': [\n",
                "        'çœ ã‚Œãªã„', 'ä¸çœ ', 'å¯ã‚Œãªã„', 'æ—©æœè¦šé†’', 'ä¸­é€”è¦šé†’', 'éçœ ', 'æ‚ªå¤¢',\n",
                "        'ä¸å®‰', 'å‹•æ‚¸', 'æ¯è‹¦ã—ã„', 'éå‘¼å¸', 'ç·Šå¼µ', 'ææ€–', 'ãƒ‘ãƒ‹ãƒƒã‚¯ç™ºä½œ',\n",
                "        'è½ã¡è¾¼ã¿', 'æ†‚ã†ã¤', 'ã‚„ã‚‹æ°—ãŒå‡ºãªã„', 'ç„¡æ°—åŠ›', 'è™šã—ã„', 'å¸Œæœ›ãŒãªã„',\n",
                "        'é ­ç—›', 'ã‚ã¾ã„', 'åãæ°—', 'é£Ÿæ¬²ä¸æŒ¯', 'é£Ÿæ¬²ãŒãªã„', 'å€¦æ€ æ„Ÿ', 'ã ã‚‹ã„', 'ç–²ã‚Œ',\n",
                "        'æ‰‹ã®éœ‡ãˆ', 'å‹•æ‚¸', 'èƒ¸ãŒè‹¦ã—ã„', 'æ¯åˆ‡ã‚Œ',\n",
                "        'é›†ä¸­ã§ããªã„', 'ç‰©å¿˜ã‚Œ', 'é ­ãŒå›ã‚‰ãªã„', 'æ±ºæ–­ã§ããªã„',\n",
                "        'æ¶™ãŒæ­¢ã¾ã‚‰ãªã„', 'æ³£ã„ã¦ã—ã¾ã†', 'ã‚¤ãƒ©ã‚¤ãƒ©', 'æ€’ã‚Š', 'æ„Ÿæƒ…ã®èµ·ä¼',\n",
                "        'äººãŒæ€–ã„', 'å¤–å‡ºã§ããªã„', 'å¼•ãã“ã‚‚ã‚Š',\n",
                "        'å¸Œæ­»å¿µæ…®', 'æ­»ã«ãŸã„', 'è‡ªå‚·', 'æ¶ˆãˆãŸã„', 'ãƒªã‚¹ãƒˆã‚«ãƒƒãƒˆ',\n",
                "        'ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒãƒƒã‚¯', 'å¹»è´', 'å¹»è¦š', 'å¦„æƒ³', 'é›¢äººæ„Ÿ'\n",
                "    ],\n",
                "    'æ‚©ã¿ãƒ»çŠ¶æ³': [\n",
                "        'ä¼‘è·', 'å¾©è·', 'é€€è·', 'ä»•äº‹', 'è·å ´', 'ä¸Šå¸', 'åŒåƒš', 'éƒ¨ä¸‹', 'è»¢è·',\n",
                "        'ãƒ‘ãƒ¯ãƒãƒ©', 'ã‚»ã‚¯ãƒãƒ©', 'ãƒ¢ãƒ©ãƒãƒ©', 'ã„ã˜ã‚', 'äººé–“é–¢ä¿‚', 'æ®‹æ¥­', 'éåŠ´',\n",
                "        'å®¶æ—', 'è¦ª', 'æ¯', 'çˆ¶', 'å¤«', 'å¦»', 'å­ä¾›', 'æ¯å­', 'å¨˜',\n",
                "        'è‚²å…', 'ä»‹è­·', 'é›¢å©š', 'çµå©š', 'å«å§‘', 'ç¾©æ¯', 'ç¾©çˆ¶',\n",
                "        'æ‹æ„›', 'æ‹äºº', 'å½¼æ°', 'å½¼å¥³', 'å‹äºº', 'å­¤ç‹¬', 'å­¤ç«‹',\n",
                "        'å­¦æ ¡', 'ä¸ç™»æ ¡', 'å—é¨“', 'é€²è·¯', 'å°±æ´»', 'ç•™å¹´', 'é€€å­¦',\n",
                "        'ã‚¹ãƒˆãƒ¬ã‚¹', 'å¼•ãã“ã‚‚ã‚Š', 'å€Ÿé‡‘', 'çµŒæ¸ˆçš„', 'DV', 'è™å¾…'\n",
                "    ],\n",
                "    'ç›¸è«‡å†…å®¹': [\n",
                "        'äºˆç´„', 'ã‚­ãƒ£ãƒ³ã‚»ãƒ«', 'å¤‰æ›´', 'åˆè¨º', 'å†è¨º', 'é€šé™¢',\n",
                "        'è¨ºæ–­æ›¸', 'ç´¹ä»‹çŠ¶', 'æ„è¦‹æ›¸', 'è¨¼æ˜æ›¸',\n",
                "        'ã‚»ã‚«ãƒ³ãƒ‰ã‚ªãƒ”ãƒ‹ã‚ªãƒ³', 'è–¬', 'å‡¦æ–¹', 'å‰¯ä½œç”¨', 'æ¸›è–¬', 'æ–­è–¬',\n",
                "        'ã‚«ã‚¦ãƒ³ã‚»ãƒªãƒ³ã‚°', 'å¿ƒç†æ¤œæŸ»', 'çŸ¥èƒ½æ¤œæŸ»', 'WAIS', 'èªçŸ¥è¡Œå‹•ç™‚æ³•', 'CBT',\n",
                "        'è²»ç”¨', 'æ–™é‡‘', 'ä¿é™º', 'è‡ªç«‹æ”¯æ´', 'éšœå®³å¹´é‡‘', 'å‚·ç—…æ‰‹å½“', 'ç”Ÿæ´»ä¿è­·',\n",
                "        'éšœå®³è€…æ‰‹å¸³', 'æ‰‹å¸³',\n",
                "        'å…¥é™¢', 'è»¢é™¢', 'ç´¹ä»‹', 'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³', 'ãƒªãƒ¢ãƒ¼ãƒˆ'\n",
                "    ]\n",
                "}\n",
                "\n",
                "synonyms = {\n",
                "    'ã†ã¤': ['ã†ã¤', 'ã†ã¤ç—…', 'é¬±', 'é¬±ç—…', 'æŠ‘ã†ã¤', 'å¤§ã†ã¤'],\n",
                "    'åŒæ¥µæ€§éšœå®³': ['åŒæ¥µæ€§éšœå®³', 'åŒæ¥µæ€§', 'èºã†ã¤', 'èºé¬±', 'æ°—åˆ†éšœå®³'],\n",
                "    'ãƒ‘ãƒ‹ãƒƒã‚¯éšœå®³': ['ãƒ‘ãƒ‹ãƒƒã‚¯éšœå®³', 'ãƒ‘ãƒ‹ãƒƒã‚¯', 'ãƒ‘ãƒ‹ãƒƒã‚¯ç™ºä½œ'],\n",
                "    'ç™ºé”éšœå®³': ['ç™ºé”éšœå®³', 'ADHD', 'æ³¨æ„æ¬ é™¥', 'ASD', 'è‡ªé–‰ç—‡', 'ã‚¢ã‚¹ãƒšãƒ«ã‚¬ãƒ¼'],\n",
                "    'ä¸çœ ': ['ä¸çœ ', 'ä¸çœ ç—‡', 'çœ ã‚Œãªã„', 'å¯ã‚Œãªã„', 'ç¡çœ éšœå®³', 'æ—©æœè¦šé†’', 'ä¸­é€”è¦šé†’'],\n",
                "    'ä¸å®‰': ['ä¸å®‰', 'ä¸å®‰æ„Ÿ', 'å¿ƒé…'],\n",
                "    'é©å¿œéšœå®³': ['é©å¿œéšœå®³'],\n",
                "    'PTSD': ['PTSD', 'å¿ƒçš„å¤–å‚·', 'ãƒˆãƒ©ã‚¦ãƒ'],\n",
                "    'å¼·è¿«æ€§éšœå®³': ['å¼·è¿«æ€§éšœå®³', 'å¼·è¿«ç—‡', 'OCD'],\n",
                "    'æ‘‚é£Ÿéšœå®³': ['æ‘‚é£Ÿéšœå®³', 'éé£Ÿ', 'æ‹’é£Ÿ', 'éé£Ÿç—‡', 'æ‹’é£Ÿç—‡'],\n",
                "}\n",
                "\n",
                "def count_keywords_with_synonyms(texts, category_keywords, synonyms_dict):\n",
                "    counts = Counter()\n",
                "    if texts.empty:\n",
                "        return counts\n",
                "        \n",
                "    for text in texts:\n",
                "        if pd.isna(text):\n",
                "            continue\n",
                "        text = str(text)\n",
                "        \n",
                "        counted_groups = set()\n",
                "        for group_name, words in synonyms_dict.items():\n",
                "            if group_name in counted_groups:\n",
                "                continue\n",
                "            for word in words:\n",
                "                if word in text and word in category_keywords:\n",
                "                    counts[group_name] += 1\n",
                "                    counted_groups.add(group_name)\n",
                "                    break\n",
                "        \n",
                "        for keyword in category_keywords:\n",
                "            in_synonym = any(keyword in words for words in synonyms_dict.values())\n",
                "            if not in_synonym and keyword in text:\n",
                "                counts[keyword] += 1\n",
                "    return counts\n",
                "\n",
                "results_unified = {}\n",
                "if content_col and len(df) > 0:\n",
                "    for category, keywords in keywords_dict.items():\n",
                "        counts = count_keywords_with_synonyms(df[content_col], keywords, synonyms)\n",
                "        results_unified[category] = counts.most_common(10)\n",
                "    print(\"âœ… ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æå®Œäº†\")\n",
                "else:\n",
                "    for category in keywords_dict.keys():\n",
                "        results_unified[category] = []\n",
                "    print(\"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãªã—ã®ãŸã‚ã‚«ãƒ†ã‚´ãƒªåˆ†æã‚¹ã‚­ãƒƒãƒ—\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Azure Text Analytics æ„Ÿæƒ…åˆ†æ\n",
                "â€»Azure APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿å®Ÿè¡Œã•ã‚Œã¾ã™"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sentiment_summary_result = None\n",
                "\n",
                "if azure_key and content_col:\n",
                "    print(\"ğŸ§  æ„Ÿæƒ…åˆ†æã‚’é–‹å§‹ã—ã¾ã™ï¼ˆAzure AI Languageï¼‰...\")\n",
                "    \n",
                "    # åˆ†æå¯¾è±¡ã®æº–å‚™ï¼ˆæ–‡å­—æ•°åˆ¶é™ãªã©ã‚’è€ƒæ…®ï¼‰\n",
                "    credential = AzureKeyCredential(azure_key)\n",
                "    text_analytics_client = TextAnalyticsClient(endpoint=AZURE_ENDPOINT, credential=credential)\n",
                "    \n",
                "    # ãƒãƒƒãƒå‡¦ç†ç”¨ã®ãƒªã‚¹ãƒˆä½œæˆ\n",
                "    docs_to_analyze = []\n",
                "    # Azureåˆ¶é™: 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆæœ€å¤§10ä»¶ã€1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæœ€å¤§5120æ–‡å­—\n",
                "    for text in df[content_col]:\n",
                "        if pd.isna(text) or str(text).strip() == \"\":\n",
                "            continue\n",
                "        docs_to_analyze.append(str(text)[:5000])\n",
                "\n",
                "    print(f\"   åˆ†æå¯¾è±¡: {len(docs_to_analyze)}ä»¶\")\n",
                "    \n",
                "    counts = {'positive': 0, 'negative': 0, 'neutral': 0, 'mixed': 0}\n",
                "    scores_sum = {'positive': [], 'negative': [], 'neutral': []}\n",
                "    \n",
                "    batch_size = 10\n",
                "    processed_count = 0\n",
                "    \n",
                "    for i in range(0, len(docs_to_analyze), batch_size):\n",
                "        batch = docs_to_analyze[i:i + batch_size]\n",
                "        try:\n",
                "            response = text_analytics_client.analyze_sentiment(batch, language=\"ja\")\n",
                "            for result in response:\n",
                "                if not result.is_error:\n",
                "                    counts[result.sentiment] += 1\n",
                "                    scores_sum['positive'].append(result.confidence_scores.positive)\n",
                "                    scores_sum['negative'].append(result.confidence_scores.negative)\n",
                "                    scores_sum['neutral'].append(result.confidence_scores.neutral)\n",
                "        except Exception as e:\n",
                "            print(f\"   âš ï¸ Batch error at {i}: {e}\")\n",
                "            \n",
                "        processed_count += len(batch)\n",
                "        if processed_count % 100 == 0:\n",
                "            print(f\"   ... {processed_count}ä»¶ å®Œäº†\")\n",
                "            \n",
                "    # å¹³å‡ã‚¹ã‚³ã‚¢\n",
                "    avg_scores = {\n",
                "        'positive': mean(scores_sum['positive']) if scores_sum['positive'] else 0,\n",
                "        'negative': mean(scores_sum['negative']) if scores_sum['negative'] else 0,\n",
                "        'neutral': mean(scores_sum['neutral']) if scores_sum['neutral'] else 0\n",
                "    }\n",
                "    \n",
                "    sentiment_summary_result = {\n",
                "        'positive_count': counts['positive'],\n",
                "        'negative_count': counts['negative'],\n",
                "        'neutral_count': counts['neutral'],\n",
                "        'mixed_count': counts['mixed'],\n",
                "        'average_scores': avg_scores\n",
                "    }\n",
                "    print(\"âœ… æ„Ÿæƒ…åˆ†æå®Œäº†\")\n",
                "    print(sentiment_summary_result)\n",
                "\n",
                "else:\n",
                "    print(\"âš ï¸ Azure APIã‚­ãƒ¼ãŒãªã„ã€ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚æ„Ÿæƒ…åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™\")\n",
                "    sentiment_summary_result = None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Firestoreã«ä¿å­˜"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰\n",
                "analysis_data = {\n",
                "    'updated_at': datetime.now().isoformat(),\n",
                "    'total_count': len(df),\n",
                "\n",
                "    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æçµæœ\n",
                "    'keywords_top20': [{'word': w, 'count': c} for w, c in noun_counts.most_common(20)],\n",
                "    'keywords_tfidf': [{'word': w, 'score': round(float(s), 4)} for w, s in tfidf_ranking[:20]],\n",
                "    'bigrams_top20': [{'words': b.replace('_', ' â†’ '), 'count': c} for b, c in bigram_counts.most_common(20)],\n",
                "    'cooccurrence': {\n",
                "        target: [{'word': w, 'count': c} for w, c in words]\n",
                "        for target, words in cooccurrence_results.items()\n",
                "    },\n",
                "\n",
                "    # ã‚«ãƒ†ã‚´ãƒªåˆ†æçµæœ\n",
                "    'disease': [{'word': w, 'count': c} for w, c in results_unified['ç—…åãƒ»è¨ºæ–­'][:10]],\n",
                "    'symptoms': [{'word': w, 'count': c} for w, c in results_unified['ç—‡çŠ¶'][:10]],\n",
                "    'concerns': [{'word': w, 'count': c} for w, c in results_unified['æ‚©ã¿ãƒ»çŠ¶æ³'][:10]],\n",
                "    'inquiry_types': [{'word': w, 'count': c} for w, c in results_unified['ç›¸è«‡å†…å®¹'][:10]],\n",
                "}\n",
                "\n",
                "# æ„Ÿæƒ…åˆ†æçµæœãŒã‚ã‚Œã°è¿½åŠ \n",
                "if sentiment_summary_result:\n",
                "    analysis_data['sentiment_summary'] = sentiment_summary_result\n",
                "\n",
                "# Firestoreã«ä¿å­˜\n",
                "doc_ref = db.collection('analysis_results').document('latest')\n",
                "doc_ref.set(analysis_data, merge=True)\n",
                "\n",
                "print(\"âœ… ã™ã¹ã¦ã®åˆ†æçµæœã‚’Firestoreã«ä¿å­˜ã—ã¾ã—ãŸï¼\")\n",
                "print(f\"\\nä¿å­˜å…ˆ: analysis_results/latest\")\n",
                "print(f\"æ›´æ–°æ—¥æ™‚: {analysis_data['updated_at']}\")\n",
                "if sentiment_summary_result:\n",
                "    print(\"âœ¨ æ„Ÿæƒ…åˆ†æãƒ‡ãƒ¼ã‚¿ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}